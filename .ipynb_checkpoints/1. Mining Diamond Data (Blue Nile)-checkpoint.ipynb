{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mining Diamond Data - Blue Nile®\n",
    "\n",
    "## (a) Introduction\n",
    "In this series of notebooks we are mining diamond data from as many sources as possible to prepare our dataset. Ultimately, we want to be doing some Machine Learning on these data - but the sccraping is just as fun! We are targeting diamond merchants on the web, starting with Blue Nile® (soz Blue Nile®... but thx for the data). \n",
    "\n",
    "In all seriousness, this data is the property of Blue Nile®, so please be respectful. I try and stick to web scraping best practises in these scripts, so if you are going to use it, please keep these in. They mostly revolve around slowing the functions down, which I realise may be frustrating, but let's keep up the standards.\n",
    "\n",
    "We'll start by importing our usual packages, packages for scraping and regular expression as well as some ad hoc ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Import packages / define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time # To help slow our functions down and time them\n",
    "import random # To assign random floats to breaks, hiding predictable patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(page_link):\n",
    "    \"\"\"\n",
    "    Scrape the targeted HTML and store as a bs object\n",
    "    \"\"\"\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content)\n",
    "    return(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    \"\"\"\n",
    "    Remove HTML tags from string.\n",
    "    \"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_headers(page_content):\n",
    "    \"\"\"\n",
    "    Retrieves the headers for our Blue Nile dataframe\n",
    "    \"\"\"\n",
    "    headers_grid = page_content.find('div',{'class':'grid-header normal-header'})\n",
    "    headers_row = headers_grid.find('div', {'class':'row'})\n",
    "    \n",
    "    # Find all headers, and remove the tags from the string\n",
    "    headers_containers = []\n",
    "    for div in headers_row.find_all('div'):\n",
    "        headers_containers.append(cleanhtml(str(div.find('span'))))\n",
    "    \n",
    "    # Remove all 'None' string values from list\n",
    "    headers = list(filter(('None').__ne__, headers_containers))\n",
    "    headers.remove('Compare')\n",
    "    \n",
    "    return(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Mine Blue Nile® dataset\n",
    "\n",
    "For reference, I denote Blue Nile® as `bn` for short.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conciseness, Blue Nile we will denote as 'bn'\n",
    "bn_link = 'https://www.bluenile.com/uk/diamond-search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('C:/Users/Edward Sims/Downloads/chromedriver.exe')\n",
    "browser.get('https://www.bluenile.com/uk/diamond-search?track=NavDiaSea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have to manually remove the cookie notice... I haven't got a working solution just yet. Any tips, I'm all ears!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncheck the 360 view option for more data\n",
    "view_checkbox = browser.find_element_by_class_name('bn-checkbox')\n",
    "view_checkbox.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open more filters\n",
    "more_filters = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[13]')\n",
    "more_filters.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open polish option\n",
    "polish_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[1]/div[1]/div/div/div')\n",
    "polish_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open symmetry option\n",
    "symmetry_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[2]/div[1]/div/div/div')\n",
    "symmetry_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open fluorescence option\n",
    "fluorescence_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[3]/div[1]/div/div')\n",
    "fluorescence_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open depth % option\n",
    "depth_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[4]/div[1]/div/div')\n",
    "depth_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open table % option\n",
    "table_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[5]/div[1]/div/div')\n",
    "table_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Open L/W Ratio option\n",
    "lw_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[6]/div[1]/div/div')\n",
    "lw_add.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "\n",
    "# Add culet column\n",
    "culet_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[8]/div[2]/button')\n",
    "culet_add.click()\n",
    "time.sleep(random.uniform(0.3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "princess_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[2]/div[3]')\n",
    "emerald_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[3]/div[3]')\n",
    "asscher_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[4]/div[3]')\n",
    "cushion_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[5]/div[3]')\n",
    "marquise_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[6]/div[3]')\n",
    "radiant_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[7]/div[3]')\n",
    "oval_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[8]/div[3]')\n",
    "pear_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[9]/div[3]')\n",
    "heart_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[10]/div[3]')\n",
    "\n",
    "princess_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "emerald_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "asscher_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "cushion_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "marquise_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "radiant_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "oval_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "pear_details.click()\n",
    "time.sleep(random.uniform(0.3,3))\n",
    "heart_details.click()\n",
    "time.sleep(random.uniform(0.3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty with scraping the table is that a maximum of 1000 results are displayed. And the prices of diamonds are hugely skewed around the £600-£2,000 price range. Now we could just increase the price range by a small amount, say, £10 at a time, but given that the maximum price is over a million, this will end up taking FOREVER. But what's more, is that even in that price range there are still sometimes too many records for the table to display. And even worse still, because the data are so skewed, we'll be looping through prices at the higher end and there will not even be anything in there to display. So basically a massive waste of time for everybody involved.\n",
    "\n",
    "What we'll do instead is do a bit of exploration first.\n",
    "\n",
    "The below will loop through price ranges and scrape the numbers of results in each interval, so we can gain a rough picture of the distribution of our dataset. This will take a little time, but should help us cut down the time our final looping function takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_price_freqs():\n",
    "    \"\"\"\n",
    "    Loops through price, scrapes and stores the \n",
    "    number of results at each range.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_num_results():\n",
    "        \"\"\"\n",
    "        Scrapes the number of results shown in the price range.    \n",
    "        \"\"\"\n",
    "        results_path = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[2]/div[4]/button[1]/span[2]')\n",
    "        # Scrape the HTML and clean\n",
    "        results_val = cleanhtml(str(BeautifulSoup(results_path.get_attribute('innerHTML'))))\n",
    "        # Strip the punctuation, and convert to integer\n",
    "        results_val = int(re.sub(r'[^\\w\\s]','', results_val))\n",
    "        return(results_val)\n",
    "\n",
    "    # Create a dataframe with our new headers\n",
    "    headers = ['price_range','freq']\n",
    "    bn_price_freq = pd.DataFrame(columns=headers)\n",
    "    \n",
    "    # Min and max price locations\n",
    "    min_price_box = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/div[1]/input[1]')\n",
    "    max_price_box = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/div[1]/input[2]')\n",
    "    \n",
    "    # Assign a default interval value\n",
    "    price_interval = 2000\n",
    "    \n",
    "    # Get the min and max values (without £ and comma values)\n",
    "    min_price_value = int(min_price_box.get_attribute('value')[1:].replace(',', ''))\n",
    "    min_price_value = min_price_value - 1 # Minus 1, so we can add 1 in the loop\n",
    "    max_price_value = int(max_price_box.get_attribute('value')[1:].replace(',', ''))\n",
    "    \n",
    "    # Find a neutral zone to click on\n",
    "    neutral = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[1]/h3')\n",
    "    \n",
    "    start = time.time()\n",
    "    # Loop through prices to limit numbers displayed\n",
    "    for min_val in range(min_price_value, max_price_value, price_interval):\n",
    "        \n",
    "        lower_price = min_val + 1 # Add 1 so there are no overlapping intervals\n",
    "        higher_price = min_val + price_interval\n",
    "        \n",
    "        # Edit max price\n",
    "        max_price_box.click()\n",
    "        max_price_box.send_keys(Keys.BACKSPACE)\n",
    "        time.sleep(2)\n",
    "        max_price_box.send_keys(str(higher_price)) \n",
    "        neutral.click()\n",
    "        time.sleep(random.uniform(0.3,4))\n",
    "        \n",
    "        # Edit min price            \n",
    "        min_price_box.click()\n",
    "        min_price_box.send_keys(Keys.BACKSPACE)\n",
    "        time.sleep(1)\n",
    "        min_price_box.send_keys(str(lower_price))\n",
    "        neutral.click()\n",
    "        time.sleep(random.uniform(0.3,3))               \n",
    "        \n",
    "        # Store the range and keep going\n",
    "        price_range = str(lower_price) + ' to ' + str(higher_price)\n",
    "        freq = get_num_results()\n",
    "        results = [price_range, freq]\n",
    "        \n",
    "        bn_price_freq = bn_price_freq.append(dict(zip(headers,results)),ignore_index=True)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return(bn_price_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the frequencies and remove all ranges with no results\n",
    "bn_price_freqs = collect_price_freqs()\n",
    "bn_price_freqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_price_freqs = pd.read_csv('bn_price_freqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_price_freqs_no_zero = bn_price_freqs[bn_price_freqs.freq != 0]\n",
    "bn_price_freqs_no_zero = bn_price_freqs_no_zero.reindex(index=bn_price_freqs_no_zero.index[::-1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102201 to 1553127\n"
     ]
    }
   ],
   "source": [
    "freq = 0\n",
    "for index, freq_val in bn_price_freqs_no_zero.iterrows():\n",
    "    if freq >= 900:\n",
    "        print(freq_val[0])\n",
    "        break\n",
    "    freq = freq + freq_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_data():\n",
    "    \"\"\"\n",
    "    Loops through all the price values, scrapes the results and stores\n",
    "    it into a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_num_results():\n",
    "        \"\"\"\n",
    "        Scrapes the number of results shown in the price range.    \n",
    "        \"\"\"\n",
    "        results_path = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[2]/div[4]/button[1]/span[2]')\n",
    "        # Scrape the HTML and clean\n",
    "        results_val = cleanhtml(str(BeautifulSoup(results_path.get_attribute('innerHTML'))))\n",
    "        # Strip the punctuation, and convert to integer\n",
    "        results_val = int(re.sub(r'[^\\w\\s]','', results_val))\n",
    "        return(results_val)    \n",
    "    \n",
    "    bn_headers = []\n",
    "    \n",
    "    # Isolate the table headers HTML\n",
    "    headers_data = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[1]/div')\n",
    "    headers_html = BeautifulSoup(headers_data.get_attribute('innerHTML'))\n",
    "    \n",
    "    # Get the header values\n",
    "    for div in headers_html.find_all('div'):\n",
    "        for header in div.find_all('span'):\n",
    "            bn_headers.append(cleanhtml(str(header)))\n",
    "    bn_headers = list(filter(('').__ne__, bn_headers))\n",
    "    bn_headers.remove('Compare')\n",
    "    \n",
    "    # Create a dataframe with our new headers\n",
    "    bn_df = pd.DataFrame(columns=bn_headers)\n",
    "    \n",
    "    # Min and max price locations\n",
    "    min_price_box = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/div[1]/input[1]')\n",
    "    max_price_box = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/div[1]/input[2]')\n",
    "    \n",
    "    \n",
    "    # Assign a default range value\n",
    "    price_range = 10\n",
    "    \n",
    "    # Get the min and max values (without £ and comma values)\n",
    "    min_price_value = int(min_price_box.get_attribute('value')[1:].replace(',', ''))\n",
    "    max_price_value = int(max_price_box.get_attribute('value')[1:].replace(',', ''))\n",
    "    \n",
    "    # Find a neutral zone to click on\n",
    "    neutral = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[1]/h3')\n",
    "    \n",
    "    # Loop through prices to limit numbers displayed\n",
    "    for min_val in range(min_price_value, 237, price_range):\n",
    "        \n",
    "        # Edit min price\n",
    "        min_price_box.click()\n",
    "        min_price_box.send_keys(Keys.BACKSPACE)\n",
    "        time.sleep(1)\n",
    "        min_price_box.send_keys(str(min_val))\n",
    "        neutral.click()\n",
    "        time.sleep(random.uniform(0.3,3))\n",
    "        \n",
    "        # Edit max price\n",
    "        max_price_box.click()\n",
    "        max_price_box.send_keys(Keys.BACKSPACE)\n",
    "        time.sleep(2)\n",
    "        max_price_box.send_keys(str(min_val+price_range)) \n",
    "        neutral.click()\n",
    "\n",
    "        time.sleep(random.uniform(0.3,4))\n",
    "        \n",
    "        table_web_source = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[2]')\n",
    "        table_html = BeautifulSoup(table_web_source.get_attribute('innerHTML'))\n",
    "        \n",
    "        # Scrape the table! First get the raw table html\n",
    "        table_rows_html = table_html.find_all('a',{'class':'grid-row row '})\n",
    "        time.sleep(random.uniform(0.3,10))\n",
    "            \n",
    "        # Then loop through each row \n",
    "        for row in table_rows_html:\n",
    "            bn_data = []\n",
    "            # And loop through each value\n",
    "            for value in row.find_all('span'):\n",
    "                bn_data.append(cleanhtml(str(value)))\n",
    "            \n",
    "            bn_data = list(filter(('').__ne__, bn_data)) # Remove all empty values\n",
    "            del bn_data[4] # Delete index 4 in list as it returns two dupe vals - unique to their HTML\n",
    "            #print(bn_data)\n",
    "            bn_df = bn_df.append(dict(zip(bn_headers,bn_data)),ignore_index=True)\n",
    "\n",
    "    \n",
    "        return(bn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_val > 999: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_value = int(min_price.get_attribute('value')[1:].replace(',', ''))\n",
    "max_price_value = int(max_price.get_attribute('value')[1:].replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "a8dae707-2c7a-4728-90dd-23e595076851"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
