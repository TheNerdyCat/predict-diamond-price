{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mining Diamond Data - Blue Nile®\n",
    "\n",
    "## (a) Introduction\n",
    "In this series of notebooks we are mining diamond data from merchants on the web, and subsequently using Machine Learning to be able to predict the price of a diamond. Diamond merchants often display data on the diamonds they are selling so people can peruse them and make a purchase online. They'll also usually have a comparison element with lots of features (the 5 C's etc.). Really though, if you're anything like me (a noob jeweller), how can you tell how the diamond is priced based on these features? I guess you'd have to take the merchants' word on it... \n",
    "\n",
    "What we need is data, and a regression algorithm looking at price. This notebook is the first in the series, and in it we'll tackle scraping Blue Nile® data from [their website](https://www.bluenile.com/uk/diamond-search) (soz Blue Nile... but thx for the data). \n",
    "\n",
    "In all seriousness, this data is the property of Blue Nile®, so please be respectful. I try and stick to web scraping best practises in these scripts, so if you are going to use it, please keep these in. They mostly revolve around slowing the functions down, which I realise may be frustrating, but let's keep to the code people.\n",
    "\n",
    "We'll start by importing our packages and defining a couple of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Import packages / define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [12, 7] # Change default fig size\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import itertools\n",
    "import time # To help slow our functions down and time them\n",
    "import random # To assign random floats to breaks, hiding predictable patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some useful functions that we'll be utilising a lot in the notebook. The last two functions are essential to working with HTML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_random(start=0.6, stop=2):\n",
    "    \"\"\"\n",
    "    Pause the function for a random amount of time between the two integers entered.\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(start, stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(page_link):\n",
    "    \"\"\"\n",
    "    Scrape the targeted HTML and store as a bs object\n",
    "    \"\"\"\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content)\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    \"\"\"\n",
    "    Remove HTML tags from string.\n",
    "    \"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Prepare Blue Nile® dataset\n",
    "\n",
    "In this section we create a function to prepare the Blue Nile table, and do some intial exploration to help our final crawler save time.\n",
    "For reference, I denote Blue Nile® as `bn` for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conciseness, Blue Nile we will denote as 'bn'\n",
    "bn_link = 'https://www.bluenile.com/uk/diamond-search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start web driver\n",
    "browser = webdriver.Chrome('C:/Users/Edward Sims/Downloads/chromedriver.exe')\n",
    "browser.get(bn_link)\n",
    "pause_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_bn_table(link):\n",
    "    \"\"\"\n",
    "    Opens webdriver and prepares the table for scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Continue past the cookie notice if it exists\n",
    "    try:\n",
    "        cookie_continue = browser.find_element_by_xpath('/html/body/div[1]/button[3]')\n",
    "        cookie_continue.click()\n",
    "        pause_random()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If more filters is unselected, select it    \n",
    "    filter_status = browser.find_element_by_css_selector('.filters-overflow-button')\n",
    "    filter_status_str = str(filter_status.get_attribute('innerHTML'))\n",
    "    if 'More' in filter_status_str:\n",
    "        filter_status.click()\n",
    "    \n",
    "    # Add extra options if they are not already added\n",
    "    polish_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[1]/div[1]/div/div')\n",
    "    symmetry_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[2]/div[1]/div/div')\n",
    "    fluorescence_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[3]/div[1]/div/div')\n",
    "    depth_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[4]/div[1]/div/div')\n",
    "    table_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[5]/div[1]/div/div')\n",
    "    lw_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[6]/div[1]/div/div')\n",
    "    \n",
    "    feature_add_all = [polish_add, symmetry_add, fluorescence_add, depth_add, table_add, lw_add]\n",
    "    for feature_add in feature_add_all:\n",
    "        if 'toggled' not in str(feature_add.get_attribute('outerHTML')):\n",
    "            feature_add.click()\n",
    "            pause_random()\n",
    "            \n",
    "    culet_add = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[12]/div[8]/div[2]/button')\n",
    "    if 'active' not in str(culet_add.get_attribute('outerHTML')):\n",
    "        culet_add.click()\n",
    "        pause_random()\n",
    "    \n",
    "    # Add in all types of shape\n",
    "    round_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[1]/div[3]')\n",
    "    princess_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[2]/div[3]')\n",
    "    emerald_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[3]/div[3]')\n",
    "    asscher_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[4]/div[3]')\n",
    "    cushion_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[5]/div[3]')\n",
    "    marquise_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[6]/div[3]')\n",
    "    radiant_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[7]/div[3]')\n",
    "    oval_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[8]/div[3]')\n",
    "    pear_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[9]/div[3]')\n",
    "    heart_details = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/div[1]/div[2]/div[3]/div[6]/div[2]/div/div[10]/div[3]')\n",
    "    \n",
    "    shape_details_all = [round_details, princess_details, emerald_details, asscher_details, cushion_details, \n",
    "                         marquise_details, radiant_details, oval_details, pear_details, heart_details]\n",
    "    \n",
    "    for shape_details in shape_details_all:\n",
    "        if 'selected' not in str(shape_details.get_attribute('outerHTML')):\n",
    "            shape_details.click()\n",
    "            pause_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and prepare the bn table for scraping\n",
    "prep_bn_table(bn_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Scrape the data\n",
    "The difficulty with scraping the table is that a maximum of 1,000 results are displayed. And the prices of diamonds are hugely skewed around the £600-£2,000 price range. \n",
    "\n",
    "So we've arrived at our first major problem: \n",
    " - If we increment our price by static small amounts, it'll take weeks (umm no thanks).\n",
    " - If we increment them by static large amounts, we'll miss out loads of data from the price ranges with high frequencies.\n",
    "\n",
    "My solution below follows this method:\n",
    " 1. Retrieve the headers for our table\n",
    " 2. Create an initial price interval\n",
    " 3. Check the number of results displayed.\n",
    " 4. If more than 999, make an estimate on the sub intervals that will approximately yield 999 or less results. Scrape the table in each sub interval.\n",
    " 5. If under 999, scrape the table.\n",
    " 6. Tracking the cumulative number of results, once there are less than 999 results left, skip to the maximum price and scrape the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "min_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[1]/input')\n",
    "max_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[2]/input')\n",
    "neutral = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[1]/div[1]/h1')\n",
    "\n",
    "max_price_box.click()\n",
    "pause_random(2,3)\n",
    "#max_price_box.send_keys(Keys.BACKSPACE)\n",
    "#pause_random(2,3)\n",
    "max_price_box.send_keys('300')\n",
    "pause_random(2,3)\n",
    "\n",
    "try:\n",
    "    min_price_box.click()\n",
    "    pause_random(2,3)\n",
    "    min_price_box.send_keys(Keys.BACKSPACE)\n",
    "    pause_random(2,3)\n",
    "    min_price_box.send_keys('201')\n",
    "    pause_random(2,3)\n",
    "except StaleElementReferenceException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val: 201\n",
      "lower_price: 202\n",
      "higher_price: 301\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=79.0.3945.88)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-8b05c8d38c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mmin_price_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mmin_price_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBACKSPACE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=79.0.3945.88)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-8b05c8d38c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mpause_random\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStaleElementReferenceException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mmin_price_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mmin_price_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBACKSPACE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mmin_price_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=79.0.3945.88)\n"
     ]
    }
   ],
   "source": [
    "def get_num_results():\n",
    "    \"\"\"\n",
    "    Scrapes the number of results shown in the price range.    \n",
    "    \"\"\"\n",
    "    results_path = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[3]/div[2]/button[1]/span[2]')\n",
    "    # Scrape HTML and clean\n",
    "    results_val = cleanhtml(str(BeautifulSoup(results_path.get_attribute('innerHTML'))))\n",
    "    # Strip  punctuation, and convert to integer\n",
    "    results_val = int(re.sub(r'[^\\w\\s]','', results_val))\n",
    "    return(results_val)\n",
    "\n",
    "start = time.time()\n",
    "bn_headers = []\n",
    "# Isolate the table headers HTML\n",
    "headers_data = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[1]/div')\n",
    "headers_html = BeautifulSoup(headers_data.get_attribute('innerHTML'))\n",
    "\n",
    "# Get the header values\n",
    "for div in headers_html.find_all('div'):\n",
    "    for header in div.find_all('span'):\n",
    "        bn_headers.append(cleanhtml(str(header)))\n",
    "bn_headers = list(filter(('').__ne__, bn_headers))\n",
    "# Create a dataframe with our new headers\n",
    "bn_df = pd.DataFrame(columns = bn_headers)  \n",
    "\n",
    "# Min and max price locations\n",
    "min_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[1]/input')\n",
    "max_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[2]/input')\n",
    "\n",
    "# Assign default interval value\n",
    "price_interval = 100\n",
    "\n",
    "# Get min and max values (without £ and comma values)\n",
    "min_price_value = int(min_price_box.get_attribute('value').replace(',', ''))\n",
    "min_price_value = min_price_value - 1 # Minus 1, so we can add 1 in the loop\n",
    "max_price_value = int(max_price_box.get_attribute('value').replace(',', ''))\n",
    "# Find a neutral zone to click on\n",
    "neutral = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[1]/div[1]/h1')\n",
    "\n",
    "total_freq = get_num_results()\n",
    "cumul_freq = 0 # To cumulitively add the freqs as we go\n",
    "\n",
    "def bn_scrape_table():\n",
    "    bn_table = pd.DataFrame(columns = bn_headers)\n",
    "    table_web_source = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[2]')\n",
    "    table_html = BeautifulSoup(table_web_source.get_attribute('innerHTML'))\n",
    "    # Scrape the table! First get the raw table html\n",
    "    table_rows_html = table_html.find_all('a',{'class':'grid-row row '})\n",
    "    # Then loop through each row \n",
    "    for row in table_rows_html:\n",
    "        bn_data = []\n",
    "        # And loop through each value\n",
    "        for value in row.find_all('span'):\n",
    "            bn_data.append(cleanhtml(str(value)))\n",
    "        bn_data = list(filter(('').__ne__, bn_data)) # Remove all empty values\n",
    "        del bn_data[4] # Delete index 4 in list as it returns two dupe vals - unique to their HTML\n",
    "        \n",
    "        bn_dict = dict(zip(bn_headers, bn_data))\n",
    "        bn_table = bn_table.append(bn_dict, ignore_index=True)\n",
    "    return bn_table\n",
    "\n",
    "# Loop through prices\n",
    "for min_val in range(min_price_value, 1000, price_interval):\n",
    "    print(f'min_val: {min_val}')\n",
    "    lower_price = min_val + 1 # Add 1 so there are no overlapping intervals\n",
    "    higher_price = min_val + price_interval\n",
    "    print(f'lower_price: {lower_price}')\n",
    "    print(f'higher_price: {higher_price}')\n",
    "    \n",
    "    # Edit max price\n",
    "    try:\n",
    "        max_price_box.click()\n",
    "        max_price_box.send_keys(Keys.BACKSPACE)\n",
    "        max_price_box.send_keys(str(higher_price)) \n",
    "        neutral.click()\n",
    "        pause_random(2,3)\n",
    "    except StaleElementReferenceException:\n",
    "        max_price_box.click()\n",
    "        max_price_box.send_keys(Keys.BACKSPACE)\n",
    "        max_price_box.send_keys(str(higher_price)) \n",
    "        neutral.click()\n",
    "        pause_random(2,3)    \n",
    "    # Edit min price            \n",
    "    try:\n",
    "        min_price_box.click()\n",
    "        min_price_box.send_keys(Keys.BACKSPACE)\n",
    "        min_price_box.send_keys(str(lower_price))\n",
    "        neutral.click()\n",
    "        pause_random(2,3) \n",
    "    except StaleElementReferenceException:\n",
    "        min_price_box.click()\n",
    "        min_price_box.send_keys(Keys.BACKSPACE)\n",
    "        min_price_box.send_keys(str(lower_price))\n",
    "        neutral.click()\n",
    "        pause_random(2,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_data():\n",
    "    \"\"\"\n",
    "    Loops through all the price values, scrapes the results and stores\n",
    "    it into a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_num_results():\n",
    "        \"\"\"\n",
    "        Scrapes the number of results shown in the price range.    \n",
    "        \"\"\"\n",
    "        results_path = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[3]/div[2]/button[1]/span[2]')\n",
    "        # Scrape HTML and clean\n",
    "        results_val = cleanhtml(str(BeautifulSoup(results_path.get_attribute('innerHTML'))))\n",
    "        # Strip  punctuation, and convert to integer\n",
    "        results_val = int(re.sub(r'[^\\w\\s]','', results_val))\n",
    "        return(results_val)\n",
    "    \n",
    "    start = time.time()\n",
    "    bn_headers = []\n",
    "    # Isolate the table headers HTML\n",
    "    headers_data = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[1]/div')\n",
    "    headers_html = BeautifulSoup(headers_data.get_attribute('innerHTML'))\n",
    "    \n",
    "    # Get the header values\n",
    "    for div in headers_html.find_all('div'):\n",
    "        for header in div.find_all('span'):\n",
    "            bn_headers.append(cleanhtml(str(header)))\n",
    "    bn_headers = list(filter(('').__ne__, bn_headers))\n",
    "    # Create a dataframe with our new headers\n",
    "    bn_df = pd.DataFrame(columns = bn_headers)  \n",
    "    \n",
    "    # Min and max price locations\n",
    "    min_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[1]/input')\n",
    "    max_price_box = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[3]/div[7]/div[2]/div/fieldset/div/div[2]/input')\n",
    "    \n",
    "    # Assign default interval value\n",
    "    price_interval = 1000\n",
    "    \n",
    "    # Get min and max values (without £ and comma values)\n",
    "    min_price_value = int(min_price_box.get_attribute('value').replace(',', ''))\n",
    "    min_price_value = min_price_value - 1 # Minus 1, so we can add 1 in the loop\n",
    "    max_price_value = int(max_price_box.get_attribute('value').replace(',', ''))\n",
    "    # Find a neutral zone to click on\n",
    "    neutral = browser.find_element_by_xpath('/html/body/div[2]/main/div/div/div/div/section[1]/div[1]/div[2]/div[1]/div[1]/h1')\n",
    "    \n",
    "    total_freq = get_num_results()\n",
    "    cumul_freq = 0 # To cumulitively add the freqs as we go\n",
    "    \n",
    "    def bn_scrape_table():\n",
    "        bn_table = pd.DataFrame(columns = bn_headers)\n",
    "        table_web_source = browser.find_element_by_xpath('//*[@id=\"react-app\"]/div/div/div/section[1]/section/div/div/div[2]')\n",
    "        table_html = BeautifulSoup(table_web_source.get_attribute('innerHTML'))\n",
    "        # Scrape the table! First get the raw table html\n",
    "        table_rows_html = table_html.find_all('a',{'class':'grid-row row '})\n",
    "        # Then loop through each row \n",
    "        for row in table_rows_html:\n",
    "            bn_data = []\n",
    "            # And loop through each value\n",
    "            for value in row.find_all('span'):\n",
    "                bn_data.append(cleanhtml(str(value)))\n",
    "            bn_data = list(filter(('').__ne__, bn_data)) # Remove all empty values\n",
    "            del bn_data[4] # Delete index 4 in list as it returns two dupe vals - unique to their HTML\n",
    "            \n",
    "            bn_dict = dict(zip(bn_headers, bn_data))\n",
    "            bn_table = bn_table.append(bn_dict, ignore_index=True)\n",
    "        return bn_table\n",
    "    \n",
    "    # Loop through prices\n",
    "    for min_val in range(min_price_value, 1000, price_interval):\n",
    "        \n",
    "        lower_price = min_val + 1 # Add 1 so there are no overlapping intervals\n",
    "        higher_price = min_val + price_interval\n",
    "        \n",
    "        # Edit max price\n",
    "        max_price_box.click()\n",
    "        max_price_box.send_keys(Keys.BACKSPACE)\n",
    "        max_price_box.send_keys(str(higher_price)) \n",
    "        neutral.click()\n",
    "        pause_random(2,3)\n",
    "        # Edit min price            \n",
    "        min_price_box.click()\n",
    "        min_price_box.send_keys(Keys.BACKSPACE)\n",
    "        min_price_box.send_keys(str(lower_price))\n",
    "        neutral.click()\n",
    "        pause_random(2,3)\n",
    "        \n",
    "        freq = get_num_results()\n",
    "        cumul_freq = cumul_freq + freq \n",
    "        \n",
    "        if (total_freq - cumul_freq) <= 999:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            pause_random(7,8) # Wait for the table to load\n",
    "            \n",
    "            bn_df = bn_df.append(bn_scrape_table(), ignore_index=True)\n",
    "            \n",
    "            # Edit max price\n",
    "            max_price_box.click()\n",
    "            max_price_box.send_keys(Keys.BACKSPACE)\n",
    "            max_price_box.send_keys(str(max_price_value)) \n",
    "            neutral.click()\n",
    "            time.sleep(random.uniform(0,1))\n",
    "            # Edit min price            \n",
    "            min_price_box.click()\n",
    "            min_price_box.send_keys(Keys.BACKSPACE)\n",
    "            min_price_box.send_keys(str(lower_price + price_interval))\n",
    "            neutral.click()\n",
    "            time.sleep(random.uniform(0,1))\n",
    "            \n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            pause_random(7,8) # Wait for the table to load\n",
    "            \n",
    "            bn_df = bn_df.append(bn_scrape_table(), ignore_index=True)\n",
    "            break \n",
    "        else:           \n",
    "            # If there are over 999 results, divide the interval into smaller\n",
    "            # intervals that approximately return 999 or less results for each.\n",
    "            if freq > 999:\n",
    "                sub_interval_number = freq / 999\n",
    "                sub_interval_price = round(price_interval / sub_interval_number)\n",
    "                \n",
    "                sub_min_price_value = lower_price - 1\n",
    "                sub_max_price_value = higher_price\n",
    "                \n",
    "                for min_val in range(sub_min_price_value, sub_max_price_value, sub_interval_price):\n",
    "                    \n",
    "                    sub_lower_price = min_val + 1 # Add 1 so there are no overlapping intervals\n",
    "                    sub_higher_price = min_val + sub_interval_price\n",
    "                    \n",
    "                    # Edit max price\n",
    "                    max_price_box.click()\n",
    "                    max_price_box.send_keys(Keys.BACKSPACE)\n",
    "                    max_price_box.send_keys(str(sub_higher_price)) \n",
    "                    neutral.click()\n",
    "                    time.sleep(random.uniform(0,1))\n",
    "                    # Edit min price            \n",
    "                    min_price_box.click()\n",
    "                    min_price_box.send_keys(Keys.BACKSPACE)\n",
    "                    min_price_box.send_keys(str(sub_lower_price))\n",
    "                    neutral.click()\n",
    "                    time.sleep(random.uniform(0,1))\n",
    "                    \n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    pause_random(7,8) # Wait for the table to load\n",
    "                    \n",
    "                    bn_df = bn_df.append(bn_scrape_table(), ignore_index=True)\n",
    "            \n",
    "    end = time.time()\n",
    "    print((end - start) / 60, 'mins to complete')\n",
    "    return bn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4248749415079753 mins to complete\n"
     ]
    }
   ],
   "source": [
    "bn_df = get_bn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wish List</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Price</th>\n",
       "      <th>Carat</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Fluorescence</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Table</th>\n",
       "      <th>L/W</th>\n",
       "      <th>Price/Ct</th>\n",
       "      <th>Culet</th>\n",
       "      <th>Stock No.</th>\n",
       "      <th>Dispatch Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Wish List, Shape, Price, Carat, Cut, Color, Clarity, Polish, Symmetry, Fluorescence, Depth, Table, L/W, Price/Ct, Culet, Stock No., Dispatch Date]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_df.to_csv('blue_nile_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(bn_df.shape)\n",
    "bn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "a8dae707-2c7a-4728-90dd-23e595076851"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
